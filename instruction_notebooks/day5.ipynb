{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff43800-866e-4508-ab2e-0d9c2a093932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to https://pytorch.org/get-started/locally and install for your local configuration\n",
    "# !pip install pyarrow transformers datasets [evaluate]\n",
    "# <conda/mamba> install pyarrow transformers datasets [evaluate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c7b1b3-2da2-4d55-a4d9-cb3522eaf947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tshoe/env/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, # BERTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19abfc26-5f0b-422e-98bf-3e92b4ac1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', '[MASK]', 'of', 'text']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"This is a [MASK] of text\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b41c2bea-9b0e-4a4f-9ad3-a0ca22040fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "bert = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "627d80e9-47cc-4513-bfe8-99f06ae2c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"large language models use subword tokenization, which produces oddtokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d6ff7d8-7c27-4c0f-9927-373820539a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] large language models use subword tokenization, which produces oddtokens [SEP]'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(phrase)\n",
    "tokenizer.decode(tokens[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b8d07c87-883e-4594-9fc0-7c9d3b2b40f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "239\n",
      "##48\n",
      "##75\n",
      "##14\n",
      "##28\n",
      "##0\n",
      "##9\n",
      "##8\n",
      "##13\n",
      "##0\n",
      "##5\n",
      ".\n",
      "2013\n",
      "##8\n",
      "##9\n",
      "##42\n",
      "##3\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"239487514280981305.201389423\")\n",
    "for tokid in tokens[\"input_ids\"]:\n",
    "    print(tokenizer.decode(tokid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c65bc858-f6eb-4a3f-9e51-4b6890fbdb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phrase.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0b465ac-d559-4cf3-8aab-48951b1c5b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72a4fece-02b7-42f5-9618-d2f0822717a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "large\n",
      "language\n",
      "models\n",
      "use\n",
      "sub\n",
      "##word\n",
      "token\n",
      "##ization\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for tokid in tokens[\"input_ids\"]:\n",
    "    token = tokenizer.decode(tokid)\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4af37339-71a7-4068-864a-de0cf1e08c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large language models use subword tokenization'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70375d6-371b-494c-a5b1-b447994a8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" SolidGoldMagikarp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebb9e070-1e3a-4c43-a765-490f64d75b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Then I tried to find some way of embracing my mother's ghost.\"\n",
    "inputs = tokenizer(\n",
    "    sentence, return_tensors = \"pt\", return_attention_mask = True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10b8db6c-3cfd-41b9-9bfb-37b0eda879a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2059,  1045,  2699,  2000,  2424,  2070,  2126,  1997, 23581,\n",
       "          2026,  2388,  1005,  1055,  5745,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3f146ae-1b44-437e-b76c-07fb01f339a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2059,  1045,  2699,  2000,  2424,  2070,  2126,  1997, 23581,\n",
       "          2026,  2388,  1005,  1055,  5745,  1012,   102]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7d3cae9-abd8-431e-92df-b8fa3128c2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1005, 1012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "189ab5ca-330c-49ec-af84-5c0600042dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 102]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = tokenizer(\" \\t\\n\")\n",
    "ws[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46c06ff4-838b-490e-a0b4-43dd49a13098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [SEP]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ws[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "540ede0d-4d65-406f-9b15-c8393bb742e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b65ba6a6-ae15-4b73-9014-7673e49e2986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What did I do then?\"\n",
    "with_token_types = tokenizer(question, sentence)\n",
    "with_token_types[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c347dc5-a433-4830-8265-8f05f6a26050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a01d6b43-a6bf-4f5b-a021-a85e6e63fa6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2054,  2106,  1045,  2079,  2059,  1029,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2059,  1045,  2699,  2000,  2424,  2070,  2126,  1997, 23581,\n",
       "          2026,  2388,  1005,  1055,  5745,  1012,   102]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sequence_inputs = tokenizer(\n",
    "    [question, sentence],\n",
    "    return_tensors = \"pt\",\n",
    "    return_attention_mask = True,\n",
    "    padding = \"longest\"\n",
    ")\n",
    "two_sequence_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17054160-4bdf-4922-a041-c182a5af314a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sequence_inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ed92c3e-8d76-485a-817a-1c35b56cf697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2054, 2106, 1045, 2079, 2059, 1029,  102,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sequence_inputs = tokenizer(\n",
    "    [question, sentence],\n",
    "    return_tensors = \"pt\",\n",
    "    return_attention_mask = True,\n",
    "    padding = \"max_length\"\n",
    ")\n",
    "two_sequence_inputs[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9a07b09-a8da-4903-b00d-134fc3ad18c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window size: 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Context window size:\", tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "394ca442-87c3-44ce-ba00-0835b6cdefea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10002 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "too_long = \"a \" * 10_000\n",
    "too_long_inputs = tokenizer(\n",
    "    too_long, return_tensors = \"pt\", return_attention_mask = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b6a7431-efa1-440a-8251-00c397756a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "too_long_inputs = tokenizer(\n",
    "    too_long,\n",
    "    return_tensors = \"pt\",\n",
    "    return_attention_mask = True,\n",
    "    padding = \"max_length\",\n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26910621-3427-4798-a4fa-4c75c58a7c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(too_long_inputs[\"input_ids\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99ff930a-82f9-4cda-a1c6-1593a688d51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b9b57-f187-42b7-bb5f-9cbdb7b4f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.model(device)\n",
    "AutoModel.from_pretrained(checkpoint, device = device,) # device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e245c8f-45a3-436d-8874-121b2318dd91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6dc1c3b4-6312-42f0-a1d1-5d2e3ea1d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = bert(**inputs, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e19fe3c8-ab5c-4194-9015-4a3e04b885c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1bdd848c-e5c8-449d-ab6c-835d1e426cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1813, -0.1627, -0.2402,  ..., -0.1174,  0.2389,  0.5933],\n",
       "         [-0.1219,  0.2374, -0.8745,  ...,  0.3379,  0.4232, -0.2547],\n",
       "         [ 0.3440,  0.2197, -0.0133,  ..., -0.1566,  0.2564,  0.2016],\n",
       "         ...,\n",
       "         [ 0.5548, -0.4396,  0.7075,  ...,  0.1718, -0.1337,  0.4442],\n",
       "         [ 0.5042,  0.1461, -0.2642,  ...,  0.0728, -0.4193, -0.3139],\n",
       "         [ 0.4306,  0.1996, -0.0055,  ...,  0.1924, -0.5685, -0.3189]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0140c1c-cef9-47e9-8e68-9da4815512ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape # [CLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "360ea75e-f99d-4a6b-b6ae-0b3eef9c2f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [-0.2008,  0.1479,  0.1878,  ...,  0.9505,  0.9427,  0.1835],\n",
       "          [-0.3319,  0.4860, -0.1578,  ...,  0.5669,  0.7301,  0.1399],\n",
       "          ...,\n",
       "          [-0.1509,  0.1222,  0.4894,  ...,  0.0128, -0.1437, -0.0780],\n",
       "          [-0.3884,  0.6414,  0.0598,  ...,  0.6821,  0.3488,  0.7101],\n",
       "          [-0.5870,  0.2658,  0.0439,  ..., -0.1067, -0.0729, -0.0851]]]),\n",
       " tensor([[[-0.0422,  0.0229, -0.2086,  ...,  0.1785, -0.0790, -0.0525],\n",
       "          [-0.5901,  0.1755, -0.0278,  ...,  1.0815,  1.6212,  0.1523],\n",
       "          [ 0.0323,  0.8927, -0.2348,  ...,  0.0032,  1.3259,  0.2274],\n",
       "          ...,\n",
       "          [ 0.6683,  0.2020, -0.0523,  ...,  0.0027, -0.2793,  0.1329],\n",
       "          [-0.1310,  0.5102, -0.1028,  ...,  0.3445,  0.0718,  0.6305],\n",
       "          [-0.3432,  0.2476, -0.0468,  ..., -0.1301,  0.1246,  0.0411]]]),\n",
       " tensor([[[-0.1382, -0.2264, -0.4627,  ...,  0.3514,  0.0516, -0.0463],\n",
       "          [-0.8300,  0.4672, -0.2483,  ...,  1.2602,  1.2012, -0.1328],\n",
       "          [ 0.7289,  0.6790, -0.3091,  ..., -0.1309,  0.9835, -0.2290],\n",
       "          ...,\n",
       "          [ 0.8956,  0.3428,  0.0079,  ...,  0.2997, -0.3415,  0.7970],\n",
       "          [-0.1553,  0.2835,  0.2071,  ...,  0.0758, -0.0326,  0.6186],\n",
       "          [-0.3426,  0.0535,  0.0638,  ...,  0.0197,  0.1122, -0.1884]]]),\n",
       " tensor([[[-0.0770, -0.3675, -0.2666,  ...,  0.3117,  0.2467,  0.1323],\n",
       "          [-0.3731, -0.0286, -0.1670,  ...,  0.6970,  1.5362, -0.3529],\n",
       "          [ 0.7061,  0.4618, -0.2415,  ..., -0.0807,  0.8768, -0.2853],\n",
       "          ...,\n",
       "          [ 1.3325,  0.1663, -0.0099,  ...,  0.1685, -0.1381,  0.6110],\n",
       "          [-0.3374,  0.1269,  0.1817,  ..., -0.0198, -0.0905,  0.3292],\n",
       "          [-0.0850, -0.0934,  0.1007,  ...,  0.0459,  0.0579, -0.0371]]]),\n",
       " tensor([[[ 0.0599, -0.7039, -0.8094,  ...,  0.4053,  0.2542,  0.5017],\n",
       "          [-0.7397, -0.5218, -0.1666,  ...,  0.6768,  1.5843, -0.2920],\n",
       "          [ 0.8869,  0.5469, -0.3197,  ..., -0.0870,  0.5288,  0.1315],\n",
       "          ...,\n",
       "          [ 1.5591,  0.2863,  0.2924,  ...,  0.4971, -0.0800,  0.7023],\n",
       "          [-0.3145,  0.1553, -0.0974,  ..., -0.1852, -0.3847,  0.5292],\n",
       "          [-0.0261, -0.0488,  0.0042,  ...,  0.0081,  0.0475, -0.0346]]]),\n",
       " tensor([[[-0.0289, -0.7001, -0.6573,  ..., -0.0254,  0.2115,  0.5060],\n",
       "          [-0.9080, -0.4675, -0.2327,  ...,  0.2051,  1.5554, -0.3402],\n",
       "          [ 1.0436,  0.5098, -0.4004,  ..., -0.4537,  0.3073,  0.5464],\n",
       "          ...,\n",
       "          [ 1.8741,  0.1041, -0.1578,  ...,  0.5090,  0.0933,  0.9344],\n",
       "          [ 0.2248,  0.2398, -0.3275,  ..., -0.2687, -0.5662,  0.7646],\n",
       "          [-0.0183, -0.0432,  0.0123,  ...,  0.0138,  0.0110, -0.0385]]]),\n",
       " tensor([[[ 0.1700, -0.9118, -0.5099,  ..., -0.2153,  0.4185,  0.3388],\n",
       "          [-0.5750, -0.5454, -0.3029,  ..., -0.1316,  1.3756, -0.3223],\n",
       "          [ 0.8847,  0.6076, -0.5053,  ..., -0.5245,  0.0685,  0.3392],\n",
       "          ...,\n",
       "          [ 1.8617, -0.1778,  0.0593,  ..., -0.1164,  0.1354,  1.5028],\n",
       "          [ 0.3238,  0.6568, -0.6567,  ..., -0.6430, -0.4393,  0.4841],\n",
       "          [ 0.0172, -0.0527, -0.0179,  ..., -0.0102, -0.0174, -0.0409]]]),\n",
       " tensor([[[ 0.3411, -0.8139, -0.7188,  ..., -0.6404,  0.2390,  0.1338],\n",
       "          [-0.6435, -0.1589, -0.1621,  ..., -0.0504,  0.9217, -0.4096],\n",
       "          [ 0.7229,  0.5266, -0.7379,  ..., -0.5187,  0.0021,  0.3104],\n",
       "          ...,\n",
       "          [ 1.7987,  0.0404,  0.1860,  ..., -0.3626,  0.4451,  1.3464],\n",
       "          [ 0.1577, -0.0492, -1.1795,  ..., -0.8191, -0.4314,  0.3754],\n",
       "          [ 0.0079, -0.0187, -0.0308,  ..., -0.0261,  0.0054, -0.0522]]]),\n",
       " tensor([[[ 0.2597, -0.5194, -0.8438,  ..., -0.6873, -0.1183,  0.4508],\n",
       "          [-0.5360,  0.0884, -0.3540,  ..., -0.2608,  0.5271, -0.4311],\n",
       "          [ 0.3990,  0.4642, -0.6246,  ..., -0.5714,  0.1685,  0.5618],\n",
       "          ...,\n",
       "          [ 1.3260, -0.1660,  0.4866,  ...,  0.1439,  0.5888,  0.9798],\n",
       "          [-0.2248, -0.3549, -1.2145,  ..., -0.7236, -0.3995,  0.3148],\n",
       "          [ 0.0038, -0.0030,  0.0181,  ..., -0.0527, -0.0362, -0.0885]]]),\n",
       " tensor([[[ 0.2711, -0.3491, -0.6618,  ..., -0.1569,  0.0043,  0.3841],\n",
       "          [-0.4096,  0.3449, -0.8822,  ...,  0.2367,  0.2244, -0.4131],\n",
       "          [ 0.4250,  0.4963, -0.3541,  ..., -0.4456,  0.2106,  0.3286],\n",
       "          ...,\n",
       "          [ 1.1249, -0.2633,  0.2771,  ...,  0.2688,  0.2323,  0.7970],\n",
       "          [ 0.1102,  0.2645, -0.9370,  ..., -0.3904, -0.3523,  0.1010],\n",
       "          [-0.0321, -0.0416,  0.0300,  ..., -0.0738, -0.0530, -0.0741]]]),\n",
       " tensor([[[-0.0167, -0.2538, -0.4799,  ..., -0.0870, -0.4391,  0.3460],\n",
       "          [-0.2158,  0.3668, -0.8787,  ...,  0.1046, -0.1264, -0.5901],\n",
       "          [ 0.4833,  0.1214,  0.0037,  ..., -0.4762,  0.0543,  0.2185],\n",
       "          ...,\n",
       "          [ 0.8555, -0.2857,  0.6263,  ...,  0.5248,  0.1679,  0.6346],\n",
       "          [ 0.0267,  0.0116, -0.0948,  ..., -0.0126, -0.0193,  0.0141],\n",
       "          [-0.0377, -0.0243,  0.1689,  ...,  0.2037, -0.1910, -0.1169]]]),\n",
       " tensor([[[ 0.0439, -0.2886, -0.5210,  ..., -0.0585,  0.0057,  0.3484],\n",
       "          [ 0.2003,  0.1950, -0.8941,  ...,  0.2855,  0.3792, -0.4433],\n",
       "          [ 0.6422,  0.2077, -0.0531,  ..., -0.2940,  0.1614,  0.3406],\n",
       "          ...,\n",
       "          [ 0.8555, -0.3486,  0.6021,  ...,  0.2175,  0.1230,  0.5547],\n",
       "          [ 0.0507,  0.0111, -0.0194,  ...,  0.0255, -0.0229,  0.0141],\n",
       "          [ 0.0348, -0.0095, -0.0098,  ...,  0.0583, -0.0379, -0.0241]]]),\n",
       " tensor([[[-0.1813, -0.1627, -0.2402,  ..., -0.1174,  0.2389,  0.5933],\n",
       "          [-0.1219,  0.2374, -0.8745,  ...,  0.3379,  0.4232, -0.2547],\n",
       "          [ 0.3440,  0.2197, -0.0133,  ..., -0.1566,  0.2564,  0.2016],\n",
       "          ...,\n",
       "          [ 0.5548, -0.4396,  0.7075,  ...,  0.1718, -0.1337,  0.4442],\n",
       "          [ 0.5042,  0.1461, -0.2642,  ...,  0.0728, -0.4193, -0.3139],\n",
       "          [ 0.4306,  0.1996, -0.0055,  ...,  0.1924, -0.5685, -0.3189]]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5669598b-e8ca-4a44-869e-20ea02675ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 768])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which layer? Which token?\n",
    "outputs.hidden_states[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614a228-e44f-4f8d-8e37-5f85f878f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pooling: take the mean of all token embeddings in a sequence\n",
    "# Max pooling: takes the max for each feature for all token embeddings in a sequence\n",
    "# Mean (max, sum) of the last four layers, then use [CLS]\n",
    "# Concatenate for [CLS] all features from the last four layers: 4 * 768\n",
    "# SentenceTransformers -> n_dimensional embedding for every input sequence (dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0fa226a-2f51-46c3-9628-66994cc82cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers import pipeline, GenerationConfig\n",
    "import evaluate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93ab1fbe-ee76-45cb-a011-4b12a392af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurbs = pd.read_parquet(\"bert_blurb_classifier/blurbs.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "225ea93e-4e93-4426-ac9c-7340d696ce8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>d0</th>\n",
       "      <th>published</th>\n",
       "      <th>isbn</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Darren Wilsey, Daylle Deanna Schwartz</td>\n",
       "      <td>The Musician's Guide to Licensing Music</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>2010-02-16</td>\n",
       "      <td>9780823014873</td>\n",
       "      <td>Plug your music career into the lucrative new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Otto Karolyi</td>\n",
       "      <td>Traditional African and Oriental Music</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>1999-09-01</td>\n",
       "      <td>9780140231076</td>\n",
       "      <td>The aim of this book is to offer an introducti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Lisa Hunter</td>\n",
       "      <td>The Intrepid Art Collector</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>2006-10-24</td>\n",
       "      <td>9780307237132</td>\n",
       "      <td>Ready to upgrade your artwork from framed Mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Glyn Macey</td>\n",
       "      <td>Acrylics Unleashed</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>9781844487967</td>\n",
       "      <td>Glyn Maceys irrepressible energy and imaginati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Barry Herniman</td>\n",
       "      <td>Painting Mood and Atmosphere</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>2004-03-01</td>\n",
       "      <td>9781844480012</td>\n",
       "      <td>The glowing transparency and sponteneity of wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     d1                                 author  \\\n",
       "0  Arts & Entertainment  Darren Wilsey, Daylle Deanna Schwartz   \n",
       "1  Arts & Entertainment                           Otto Karolyi   \n",
       "2  Arts & Entertainment                            Lisa Hunter   \n",
       "3  Arts & Entertainment                             Glyn Macey   \n",
       "4  Arts & Entertainment                         Barry Herniman   \n",
       "\n",
       "                                     title          d0  published  \\\n",
       "0  The Musician's Guide to Licensing Music  Nonfiction 2010-02-16   \n",
       "1   Traditional African and Oriental Music  Nonfiction 1999-09-01   \n",
       "2               The Intrepid Art Collector  Nonfiction 2006-10-24   \n",
       "3                       Acrylics Unleashed  Nonfiction 2013-04-01   \n",
       "4             Painting Mood and Atmosphere  Nonfiction 2004-03-01   \n",
       "\n",
       "            isbn                                               text  \n",
       "0  9780823014873  Plug your music career into the lucrative new ...  \n",
       "1  9780140231076  The aim of this book is to offer an introducti...  \n",
       "2  9780307237132  Ready to upgrade your artwork from framed Mone...  \n",
       "3  9781844487967  Glyn Maceys irrepressible energy and imaginati...  \n",
       "4  9781844480012  The glowing transparency and sponteneity of wa...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blurbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "254a3c4d-36a0-48b3-a8bd-2c75ae303fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerated = list(enumerate(blurbs[\"d1\"].unique()))\n",
    "id2label = {idx: genre for idx, genre in enumerated}\n",
    "label2id = {genre: idx for idx, genre in enumerated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8084a650-3954-47f4-a1dd-55fd67c82220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Arts & Entertainment',\n",
       " 1: 'Biography & Memoir',\n",
       " 2: 'Childrenâ€™s Middle Grade Books',\n",
       " 3: 'Cooking',\n",
       " 4: 'Graphic Novels & Manga',\n",
       " 5: 'Literary Fiction',\n",
       " 6: 'Mystery & Suspense',\n",
       " 7: 'Politics',\n",
       " 8: 'Religion & Philosophy',\n",
       " 9: 'Romance'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9acd1ec5-687e-4b9c-9ecd-09cd1ee80145",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurbs[\"label\"] = blurbs[\"d1\"].replace(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e428f7b-c368-4b7f-a968-d104fd4fd0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>d0</th>\n",
       "      <th>published</th>\n",
       "      <th>isbn</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Darren Wilsey, Daylle Deanna Schwartz</td>\n",
       "      <td>The Musician's Guide to Licensing Music</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>2010-02-16</td>\n",
       "      <td>9780823014873</td>\n",
       "      <td>Plug your music career into the lucrative new ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Otto Karolyi</td>\n",
       "      <td>Traditional African and Oriental Music</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>1999-09-01</td>\n",
       "      <td>9780140231076</td>\n",
       "      <td>The aim of this book is to offer an introducti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Lisa Hunter</td>\n",
       "      <td>The Intrepid Art Collector</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>2006-10-24</td>\n",
       "      <td>9780307237132</td>\n",
       "      <td>Ready to upgrade your artwork from framed Mone...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Glyn Macey</td>\n",
       "      <td>Acrylics Unleashed</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>9781844487967</td>\n",
       "      <td>Glyn Maceys irrepressible energy and imaginati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "      <td>Barry Herniman</td>\n",
       "      <td>Painting Mood and Atmosphere</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>2004-03-01</td>\n",
       "      <td>9781844480012</td>\n",
       "      <td>The glowing transparency and sponteneity of wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     d1                                 author  \\\n",
       "0  Arts & Entertainment  Darren Wilsey, Daylle Deanna Schwartz   \n",
       "1  Arts & Entertainment                           Otto Karolyi   \n",
       "2  Arts & Entertainment                            Lisa Hunter   \n",
       "3  Arts & Entertainment                             Glyn Macey   \n",
       "4  Arts & Entertainment                         Barry Herniman   \n",
       "\n",
       "                                     title          d0  published  \\\n",
       "0  The Musician's Guide to Licensing Music  Nonfiction 2010-02-16   \n",
       "1   Traditional African and Oriental Music  Nonfiction 1999-09-01   \n",
       "2               The Intrepid Art Collector  Nonfiction 2006-10-24   \n",
       "3                       Acrylics Unleashed  Nonfiction 2013-04-01   \n",
       "4             Painting Mood and Atmosphere  Nonfiction 2004-03-01   \n",
       "\n",
       "            isbn                                               text  label  \n",
       "0  9780823014873  Plug your music career into the lucrative new ...      0  \n",
       "1  9780140231076  The aim of this book is to offer an introducti...      0  \n",
       "2  9780307237132  Ready to upgrade your artwork from framed Mone...      0  \n",
       "3  9781844487967  Glyn Maceys irrepressible energy and imaginati...      0  \n",
       "4  9781844480012  The glowing transparency and sponteneity of wa...      0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blurbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2d87d29-8641-4227-a08c-31997098d320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(blurbs[[\"text\", \"label\"]])\n",
    "blurbs[\"label\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93886b7c-3735-46fa-b8bd-dd1eea94eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels = blurbs[\"label\"].nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5892a2ce-d158-42aa-b39f-97dc9e801e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "434fe357-6e19-449b-b5c9-26e115f2ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    \"\"\"Tokenize strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict\n",
    "        Batch of texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tokenized : dict\n",
    "        Tokenized texts\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "    )\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6429d902-8bb5-421e-b5b1-8b130161875a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10942\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3648\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = dataset.train_test_split()\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4bf45f5a-1002-4c69-bc3d-683e3213243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100% 10942/10942 [00:04<00:00, 2241.18 examples/s]\n",
      "Map: 100% 3648/3648 [00:01<00:00, 1958.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainset = split[\"train\"].map(tokenize, batched = True)\n",
    "testset = split[\"test\"].map(tokenize, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3f7e359-e7b9-4dc6-b4d4-8f09ff85eb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 10942\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aeba8864-9876-4ac3-b01b-de79752df541",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "721e9c94-912a-49c3-8be0-7263af752149",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(evaluations):\n",
    "    \"\"\"Compute metrics for a set of predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evaluations : tuple\n",
    "        Model logits/label for each text and texts' true labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : dict\n",
    "        The metric scores\n",
    "    \"\"\"\n",
    "    # Split the model logits from the true labels\n",
    "    logits, references = evaluations\n",
    "\n",
    "    # Find the model prediction with the maximum value\n",
    "    predictions = np.argmax(logits, axis = 1)\n",
    "\n",
    "    # Calculate the scores\n",
    "    accuracy = accuracy_metric.compute(\n",
    "        predictions = predictions, references = references\n",
    "    )\n",
    "    f1 = f1_metric.compute(\n",
    "        predictions = predictions,\n",
    "        references = references,\n",
    "        average = \"weighted\"\n",
    "    )\n",
    "\n",
    "    # Wrap up the scores and return them for display during logging\n",
    "    scores = {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1[\"f1\"]}\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9c8d69c0-47e8-4da6-9e2c-364a69d2a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e426febc-99ea-4897-99af-c416d6c26543",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 32\n",
    "per_device_eval_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5dc7f303-bb22-40db-84b7-583a4aace87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cfd68c2e-3624-4a85-bca7-c31293019163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of warm up steps: 513\n"
     ]
    }
   ],
   "source": [
    "warmup_steps = (len(trainset) / per_device_train_batch_size) * num_train_epochs\n",
    "warmup_steps = round(warmup_steps * 0.1)\n",
    "print(\"Number of warm up steps:\", warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f9db7e2-b263-4553-a9c2-f7c31c34ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "502e5045-aa0a-4668-861f-47417607c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"data/bert_blurb_classifier\",\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "    learning_rate = learning_rate,\n",
    "    warmup_steps = warmup_steps,\n",
    "    weight_decay = weight_decay,\n",
    "    logging_steps = 100,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"loss\",\n",
    "    save_total_limit = 3,\n",
    "    push_to_hub = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e1d3cb62-db82-4ea7-bfdc-5429fb03297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    args = training_args,\n",
    "    train_dataset = trainset,\n",
    "    eval_dataset = testset,\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becae5b-8b4f-419c-82f5-d900def5cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "80bf23d2-c725-4808-aa3a-93c3af0f28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = \"bert_blurb_classifier/final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "318c49d3-a805-4359-ac8b-6401074bd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "    \"text-classification\", model = model, tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "61aab1e9-0d68-46c3-b092-5724d51b6dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Literary Fiction', 'score': 0.22967785596847534},\n",
       " {'label': 'Romance', 'score': 0.1994495838880539},\n",
       " {'label': 'Childrenâ€™s Middle Grade Books', 'score': 0.16720600426197052},\n",
       " {'label': 'Biography & Memoir', 'score': 0.16302183270454407},\n",
       " {'label': 'Arts & Entertainment', 'score': 0.0830671563744545},\n",
       " {'label': 'Mystery & Suspense', 'score': 0.05628233775496483},\n",
       " {'label': 'Politics', 'score': 0.048288144171237946},\n",
       " {'label': 'Graphic Novels & Manga', 'score': 0.032358285039663315},\n",
       " {'label': 'Religion & Philosophy', 'score': 0.012297534383833408},\n",
       " {'label': 'Cooking', 'score': 0.008351278491318226}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = blurbs.sample(1)\n",
    "#print(sample[\"text\"].item())\n",
    "classifier(\"1930s Europe was a tumultous time\", top_k = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fd6a2e94-e275-4c5b-b5eb-7cdfd58f612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"gpt2\"\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "gpt = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a2e08834-be48-451c-a548-9d718adcd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"It was the best of times, it was the\"\n",
    "tokenized = gpt_tokenizer(prompt, return_tensors = \"pt\", truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7976b3d4-950a-4446-b4b1-ad0e362d5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated = gpt(**tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8c30c748-9389-432e-9b56-844339f2c46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 50257])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0893380c-1a0b-488c-994c-ab30d15ddada",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_logits = generated.logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b8e7e726-39c4-472c-92cf-dbd1dac4b593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -95.6606,  -94.0251,  -98.6486,  ...,  -98.9755, -100.9134,\n",
       "          -94.9695]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "716d171f-aeac-417a-92af-79a882a3f6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57611688, 0.21194156, 0.21194156])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [2.0, 1.0, 1.0]\n",
    "exponentiated = [np.exp(val) for val in z]\n",
    "summed = np.sum(exponentiated)\n",
    "\n",
    "exponentiated / summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c985c06e-65bc-4027-93d1-ba689b0fa441",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(last_token_logits, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d5aa3697-2554-40e7-bc80-36f5b1e7198e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8ee77daf-66ec-493c-9906-2c4cd5ffdde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6413e-07, 1.8687e-06, 1.8348e-08, 6.0486e-09, 1.2955e-07, 3.2124e-08,\n",
       "        9.7930e-07, 3.6522e-07, 2.3999e-06, 9.9195e-08])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fc4f6fb6-ada4-40ac-a6a1-d55d3683db8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next predicted token:  worst\n"
     ]
    }
   ],
   "source": [
    "next_token_id = torch.argmax(probs).item()\n",
    "next_token = gpt_tokenizer.decode(next_token_id)\n",
    "\n",
    "print(\"Next predicted token:\", next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "97cb26a3-ed2b-4bd4-959f-5f644a1c654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full sequence: It was the best of times, it was the worst of times.\n",
      "\n",
      "\"I'm not\n"
     ]
    }
   ],
   "source": [
    "generated_token_ids = gpt.generate(**tokenized, max_new_tokens = 10)\n",
    "generated = gpt_tokenizer.decode(generated_token_ids.squeeze())\n",
    "\n",
    "print(\"Full sequence:\", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb6b86-4db0-40aa-a8a2-bae196e29e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [\"tokens\"]\n",
    "for i in range(4):\n",
    "    current = seq.copy()\n",
    "    output = gpt(current)\n",
    "    seq = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4c0e90e2-452d-4c3f-b8ad-3926f793452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model = gpt, tokenizer = gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9a1cfa1d-4bf4-4f1e-93cf-126f77054b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated ,\u001b[38;5;241m=\u001b[39m generator(prompt, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, do_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, num_return_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 1)"
     ]
    }
   ],
   "source": [
    "generated ,= generator(prompt, max_new_tokens = 4, do_sample = True, num_return_sequences = 5)\n",
    "print(generated[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8d1a28c7-5b10-4580-8b2c-b0462bdb143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'It was the best of times, it was the last of years,'},\n",
       " {'generated_text': 'It was the best of times, it was the best of times.'},\n",
       " {'generated_text': 'It was the best of times, it was the best of times,\"'},\n",
       " {'generated_text': 'It was the best of times, it was the worst of times,\"'},\n",
       " {'generated_text': 'It was the best of times, it was the best of times.\"'}]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt, max_new_tokens = 4, do_sample = True, num_return_sequences = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c52277b2-0c61-447a-9dd4-eb2878314bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times, it was the worst. We were\n",
      "It was the best of times, it was the best of times,\"\n",
      "It was the best of times, it was the best of times.\n",
      "It was the best of times, it was the worst. I'd\n",
      "It was the best of times, it was the most wonderful of times\n"
     ]
    }
   ],
   "source": [
    "generated = generator(\n",
    "    prompt,\n",
    "    do_sample = True,\n",
    "    max_new_tokens = 4,\n",
    "    top_k = 50,\n",
    "    num_return_sequences = 5\n",
    ")\n",
    "for seq in generated:\n",
    "    print(seq[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ca34aeb5-94e8-4b0d-974f-febb027017a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times, it was the worst of times,\n",
      "It was the best of times, it was the best of times,\"\n",
      "It was the best of times, it was the worst.\n",
      "\n",
      "\n",
      "It was the best of times, it was the worst of times.\n",
      "It was the best of times, it was the worst of times,\n"
     ]
    }
   ],
   "source": [
    "generated = generator(\n",
    "    prompt,\n",
    "    do_sample = True,\n",
    "    max_new_tokens = 4,\n",
    "    top_p = 0.9,\n",
    "    num_return_sequences = 5\n",
    ")\n",
    "for seq in generated:\n",
    "    print(seq[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "64717092-20f1-4b2d-b7e1-cc0f6560e89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times, it was the saddlin,' Paul says\n",
      "It was the best of times, it was the very toughest stuff imaginable --\n",
      "It was the best of times, it was the finest!\" he remarked triumph\n",
      "It was the best of times, it was the right place\n",
      "Ek\n",
      "It was the best of times, it was the least to miss though (\n"
     ]
    }
   ],
   "source": [
    "generated = generator(\n",
    "    prompt, \n",
    "    do_sample = True,\n",
    "    max_new_tokens = 5,\n",
    "    temperature = 50.0,\n",
    "    num_return_sequences = 5\n",
    ")\n",
    "for seq in generated:\n",
    "    print(seq[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0292cc5d-1fce-4949-997d-b4bc13fe94cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "gpt_tokenizer.pad_token_id = 50256\n",
    "print(\"Pad token:\", gpt_tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "55a2c7f4-f5ec-4a14-9849-077137a5b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GenerationConfig(\n",
    "    max_new_tokens = 25,\n",
    "    do_sample = True, \n",
    "    temperature = 1.5,\n",
    "    top_p = 0.8, \n",
    "    num_return_sequences = 5,\n",
    ")\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = gpt,\n",
    "    tokenizer = gpt_tokenizer,\n",
    "    generation_config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "96fd5fa7-d994-452b-98f1-7c1be6a8b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times, it was the best of times,\" the 32-year-old defender said, as he sat in a makeshift stand next to his post in\n",
      "It was the best of times, it was the first real day of my year,\" the 21-year-old former United Nations World Cup soccer player said after the U.\n",
      "It was the best of times, it was the worst.\"\n",
      "\n",
      "Maurice Brown, a spokeswoman for Humber Valley Police, said that there were two complaints after the\n",
      "It was the best of times, it was the worst of times. So what would you do?\" He shrugged the memory away, as if taking his time with it.\n",
      "\n",
      "It was the best of times, it was the greatest of times.\"\n",
      "\n",
      "Rajesh had gone out of his way to highlight his father's legacy by being a true\n"
     ]
    }
   ],
   "source": [
    "generated = generator(prompt)\n",
    "for seq in generated:\n",
    "    print(seq[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa941f30-bd11-40c1-a9b4-578bf0257005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
